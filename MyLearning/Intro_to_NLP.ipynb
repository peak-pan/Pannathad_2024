{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEMMING AND LEMMATISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\peaks\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet') # คลังคำศัพท์ คำความหมายคล้ายกัน Synonym vocab\n",
    "from nltk.stem.porter import PorterStemmer #import stemming function\n",
    "from nltk.stem.wordnet import WordNetLemmatizer #import Lemmatisation fuction\n",
    "stemmer = PorterStemmer()\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_vs_lemma(text):\n",
    "    print(f\"{'word':<12}\\t{'lemma':<12}\\t{'stem':<12}\")\n",
    "    for word in text:\n",
    "        print(f'{word:12}\\t{lemma.lemmatize(word):12}\\t{stemmer.stem(word):12}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = ['fly', 'flies', 'flying', 'flew', 'flown']\n",
    "word_list2 = ['dance', 'dancing', 'danced',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word        \tlemma       \tstem        \n",
      "fly         \tfly         \tfli         \n",
      "flies       \tfly         \tfli         \n",
      "flying      \tflying      \tfli         \n",
      "flew        \tflew        \tflew        \n",
      "flown       \tflown       \tflown       \n"
     ]
    }
   ],
   "source": [
    "stem_vs_lemma(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word        \tlemma       \tstem        \n",
      "dance       \tdance       \tdanc        \n",
      "dancing     \tdancing     \tdanc        \n",
      "danced      \tdanced      \tdanc        \n"
     ]
    }
   ],
   "source": [
    "stem_vs_lemma(word_list2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\peaks\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_removal(text):\n",
    "    return [i for i in text if i not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"When your legs don't work like they used to before And I can't sweep you off of your feet Will your mouth still remember the taste of my love Will your eyes still smile from your cheeks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"When legs work like used And I can't sweep feet Will mouth still remember taste love Will eyes still smile cheeks\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(stopwords_removal(text1.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalisation\n",
    "-   2moro, 2mrrw , 2mrw , tomrw -> tomorrow\n",
    "-   otw -> on the way , FYI -> for your information, TTYL -> talk to you later\n",
    "-    emoji -> word , :), :D ,:-) -> smile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_dict = {'2moro':'tomorrow',\n",
    "             '2mrrw':'tomorrow',\n",
    "             '2morrow':'tomorrow',\n",
    "             'tmorw':'tomorrow',\n",
    "             'b4':'before',\n",
    "             'otw':'on the way',\n",
    "             ':)':'smile',}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nomalise(text):\n",
    "    return [norm_dict[i] for i in text if i in norm_dict.keys() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tomorrow', 'tomorrow', 'tomorrow', 'on the way']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list = ['2moro','2mrrw','2morrow','otw']\n",
    "nomalise(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noise Removal\n",
    "-   clean all noise such as # .. ? <> // "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_removal(text):\n",
    "    text = re.sub(\"(<.*?>)\",\"\",text)\n",
    "    text = re.sub(\"(\\\\W|\\\\d)\",\" \",text)\n",
    "    text = text.strip() #remove whitespace\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is  banggg'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = 'this is <a> banggg**?'\n",
    "noise_removal(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('plan.n.01'),\n",
       " Synset('program.n.02'),\n",
       " Synset('broadcast.n.02'),\n",
       " Synset('platform.n.02'),\n",
       " Synset('program.n.05'),\n",
       " Synset('course_of_study.n.01'),\n",
       " Synset('program.n.07'),\n",
       " Synset('program.n.08'),\n",
       " Synset('program.v.01'),\n",
       " Synset('program.v.02')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "synonym = wordnet.synsets(\"program\") # find synonym or related word of \"program\"\n",
    "synonym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('game.n.01'),\n",
       " Synset('game.n.02'),\n",
       " Synset('game.n.03'),\n",
       " Synset('game.n.04'),\n",
       " Synset('game.n.05'),\n",
       " Synset('game.n.06'),\n",
       " Synset('game.n.07'),\n",
       " Synset('plot.n.01'),\n",
       " Synset('game.n.09'),\n",
       " Synset('game.n.10'),\n",
       " Synset('game.n.11'),\n",
       " Synset('bet_on.v.01'),\n",
       " Synset('crippled.s.01'),\n",
       " Synset('game.s.02')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonym2 = wordnet.synsets(\"game\")\n",
    "synonym2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"เหตุเกิดจากความเหงา ที่ทำให้รู้ว่ารักเธอเท่าไหร่\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DICTIONARY BASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logest ['เหตุ', 'เกิด', 'จาก', 'ความเหงา', ' ', 'ที่', 'ทำให้', 'รู้', 'ว่า', 'รัก', 'เธอ', 'เท่าไหร่']\n",
      "maximum ['เหตุ', 'เกิด', 'จากความเหงา', ' ', 'ที่', 'ทำให้', 'รู้', 'ว่า', 'รัก', 'เธอ', 'เท่าไหร่']\n",
      "new maximum ['เหตุ', 'เกิด', 'จาก', 'ความเหงา', ' ', 'ที่', 'ทำให้', 'รู้', 'ว่า', 'รัก', 'เธอ', 'เท่าไหร่']\n",
      "new maximum-safe ['เหตุ', 'เกิด', 'จาก', 'ความเหงา', ' ', 'ที่', 'ทำให้', 'รู้', 'ว่า', 'รัก', 'เธอ', 'เท่าไหร่']\n"
     ]
    }
   ],
   "source": [
    "print(\"logest\",word_tokenize(text, engine=\"longest\"))\n",
    "print(\"maximum\",word_tokenize(text, engine=\"mm\"))\n",
    "print(\"new maximum\",word_tokenize(text, engine=\"newmm\"))\n",
    "print(\"new maximum-safe\",word_tokenize(text, engine=\"newmm-safe\"))\n",
    "#print(\"icu\",word_tokenize(text, engine=\"icu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MACHINE LEARNING BASED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\peaks\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\peaks\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "deepcut ['เหตุ', 'เกิด', 'จาก', 'ความ', 'เหงา', ' ', 'ที่', 'ทำ', 'ให้', 'รู้', 'ว่า', 'รัก', 'เธอ', 'เท่า', 'ไหร่']\n"
     ]
    }
   ],
   "source": [
    "print(\"deepcut\",word_tokenize(text, engine=\"deepcut\"))\n",
    "#print(\"attacut\",word_tokenize(text, engine=\"attacut\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 4ms/step\n",
      "deepcut ['เหตุ', 'เกิด', 'จาก', 'ความ', 'เหงา', ' ', 'ที่', 'ทำ', 'ให้', 'รู้', 'ว่า', 'รัก', 'เธอ', 'เท่า', 'ไหร่']\n"
     ]
    }
   ],
   "source": [
    "#%%timeit\n",
    "print(\"deepcut\",word_tokenize(text, engine=\"deepcut\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part Of Speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\peaks\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\peaks\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'love', 'Opall']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'I love Opall'\n",
    "token = nltk.word_tokenize(sentence)\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'), ('love', 'VBP'), ('Opall', 'RB')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'ฉันรักธรรมศาสตร์เพราะธรรมศาสตร์สอนให้ฉันรักประชาชน'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ฉัน',\n",
       " 'รัก',\n",
       " 'ธรรมศาสตร์',\n",
       " 'เพราะ',\n",
       " 'ธรรมศาสตร์',\n",
       " 'สอน',\n",
       " 'ให้',\n",
       " 'ฉัน',\n",
       " 'รัก',\n",
       " 'ประชาชน']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = word_tokenize(text,engine='newmm')\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.tag import pos_tag, pos_tag_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ฉัน', 'PPRS'),\n",
       " ('รัก', 'VACT'),\n",
       " ('ธรรมศาสตร์', 'NCMN'),\n",
       " ('เพราะ', 'JSBR'),\n",
       " ('ธรรมศาสตร์', 'NCMN'),\n",
       " ('สอน', 'VACT'),\n",
       " ('ให้', 'JSBR'),\n",
       " ('ฉัน', 'PPRS'),\n",
       " ('รัก', 'VACT'),\n",
       " ('ประชาชน', 'NCMN')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(sent) # Default engine = \"perceptron\" corpus = \"orchid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ฉัน', 'PRON'),\n",
       " ('รัก', 'VERB'),\n",
       " ('ธรรมศาสตร์', 'NOUN'),\n",
       " ('เพราะ', 'ADP'),\n",
       " ('ธรรมศาสตร์', 'NOUN'),\n",
       " ('สอน', 'VERB'),\n",
       " ('ให้', 'VERB'),\n",
       " ('ฉัน', 'PRON'),\n",
       " ('รัก', 'VERB'),\n",
       " ('ประชาชน', 'NOUN')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(sent, corpus=\"pud\") # pud = Parallel Universal Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER Name Entities Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The UK will host the 26th UN Climate Change Conference of the Parties (COP26) in Glasglow on 31 Octorber - 12 November 2021\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\peaks\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Error loading average_perceptron_tagger: Package\n",
      "[nltk_data]     'average_perceptron_tagger' not found in index\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\peaks\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\peaks\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('average_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORGANIZATION UK\n",
      "PERSON Climate Change Conference\n",
      "GPE Parties\n",
      "ORGANIZATION COP26\n",
      "GPE Glasglow\n"
     ]
    }
   ],
   "source": [
    "for sent in nltk.sent_tokenize(text):\n",
    "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "        if hasattr(chunk,'label'):\n",
    "            print(chunk.label(),' '.join(c[0] for c in chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UK                                                \tGPE            \n",
      "26th                                              \tORDINAL        \n",
      "UN Climate Change Conference of the Parties       \tEVENT          \n",
      "Glasglow                                          \tGPE            \n",
      "31                                                \tCARDINAL       \n",
      "November 2021                                     \tDATE           \n"
     ]
    }
   ],
   "source": [
    "nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text:<50}\\t{ent.label_:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\peaks\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "nlp_BERT = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'B-LOC', 'score': 0.999511, 'index': 2, 'word': 'UK', 'start': 4, 'end': 6}\n",
      "{'entity': 'B-MISC', 'score': 0.90101033, 'index': 7, 'word': 'UN', 'start': 26, 'end': 28}\n",
      "{'entity': 'I-MISC', 'score': 0.98907846, 'index': 8, 'word': 'Climate', 'start': 29, 'end': 36}\n",
      "{'entity': 'I-MISC', 'score': 0.97872615, 'index': 9, 'word': 'Change', 'start': 37, 'end': 43}\n",
      "{'entity': 'I-MISC', 'score': 0.9932528, 'index': 10, 'word': 'Conference', 'start': 44, 'end': 54}\n",
      "{'entity': 'I-MISC', 'score': 0.9913018, 'index': 11, 'word': 'of', 'start': 55, 'end': 57}\n",
      "{'entity': 'I-MISC', 'score': 0.99368155, 'index': 12, 'word': 'the', 'start': 58, 'end': 61}\n",
      "{'entity': 'I-MISC', 'score': 0.98914206, 'index': 13, 'word': 'Parties', 'start': 62, 'end': 69}\n",
      "{'entity': 'B-MISC', 'score': 0.98339003, 'index': 15, 'word': 'CO', 'start': 71, 'end': 73}\n",
      "{'entity': 'I-MISC', 'score': 0.8735296, 'index': 16, 'word': '##P', 'start': 73, 'end': 74}\n",
      "{'entity': 'I-MISC', 'score': 0.86632967, 'index': 17, 'word': '##26', 'start': 74, 'end': 76}\n",
      "{'entity': 'B-LOC', 'score': 0.997592, 'index': 20, 'word': 'G', 'start': 81, 'end': 82}\n",
      "{'entity': 'I-LOC', 'score': 0.84935707, 'index': 21, 'word': '##las', 'start': 82, 'end': 85}\n",
      "{'entity': 'I-LOC', 'score': 0.99705243, 'index': 22, 'word': '##g', 'start': 85, 'end': 86}\n",
      "{'entity': 'I-LOC', 'score': 0.9963014, 'index': 23, 'word': '##low', 'start': 86, 'end': 89}\n"
     ]
    }
   ],
   "source": [
    "result = nlp_BERT(text)\n",
    "for entity in result:\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyThaiNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pythainlp.tag.named_entity import ThaiNameTagger\n",
    "#ner = ThaiNameTagger()\n",
    "text = 'สวัสดีครับ เราเคยรู้จักกันหรือเปล่า หน้าตาคุ้นๆ แค่ผมมองคุณยังไม่ค่อยชัด'\n",
    "#ner.get_ner(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
